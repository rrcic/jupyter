{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d56a0afe-bfbd-4791-8637-b060aba9d7dd",
   "metadata": {},
   "source": [
    "# Support Vector Machines(SVM) \n",
    "## Part 0 Introduction\n",
    "The SVM is a binary classification model. For the two sets of vectors that are marked, an optimal segmentation hypersurface is given to divide the two sets of vectors into two sides, so that the vector in the two sets of vectors that is closest to the hyperplane (that is, The so-called support vectors) are as far away as possible from this hyperplane.<br /> <br /> \n",
    "SVM model types can be roughly divided into the following three types： <br /> \n",
    "（1）Linearly separable support vector machine: When the data is linearly separable, a linear classifier is learned through the hard margin maximum method, which can also be called a hard margin support vector machine.<br /> \n",
    "（2）Linear support vector machine, when the data is approximately linearly separable, learn a linear classifier through the soft margin maximum method, which can also be called soft margin support vector machine. <br /> \n",
    "（3）Nonlinear SVM: Learning a nonlinear classifier by kernel trick and soft margin maximization when the data is linearly inseparable.<br /> \n",
    "Next, we briefly introduce these three method.\n",
    "## Part 1 Principle introduction\n",
    "### 0.Function Margin & Geometric Margin\n",
    "#### a) Function Margin\n",
    "For a given dataset D and hyperplane (w, b), define the function margin of hyperplane (w, b) with respect to sample points (x<sub>i</sub>,y<sub>i</sub>) as\n",
    "<center>$\\widehat{\\gamma}$<sub>i</sub> = y<sub>i</sub>&nbsp;(w$\\cdot$x<sub>i</sub>+b)</center>\n",
    "\n",
    "\n",
    "#### b) Geometric Margin\n",
    "The geometric margin of the hyperplane (w,b) with respect to the sample points (x<sub>i</sub>, y<sub>i</sub>) is generally the signed distance from the instance point to the hyperplane. When the sample point is correctly classified by the hyperplane, it is the distance from the instance point to the hyperplane. Expressed as:\n",
    "<center>$\\gamma$<sub>i</sub> = y<sub>i</sub>&nbsp;($\\frac{w}{||w||}$$\\cdot$x<sub>i</sub>+$\\frac{b}{||w||}$)</center>\n",
    "\n",
    "\n",
    "#### c) The relationship between function margin and geometric margin\n",
    "<center>$\\gamma$<sub>i</sub> = $\\frac{\\widehat{\\gamma}_i}{||w||}$</center>\n",
    "If $||w||=1$, then the function margin is equal to the geometric margin.\n",
    "\n",
    "### 1.Hard-Margin Support Vector Machines\n",
    "Support vector machine belongs to supervised learning, and its training data appears in pairs, so given a training set D={(x<sub>1</sub>, y<sub>1</sub>), (x<sub >2</sub>, y<sub>2</sub>), ...(x<sub>m</sub>,y<sub>m</sub>)}, y<sub>i< /sub>=(+1, -1) must appear in pairs. The learning goal of the linear separable SVM is to find a separating hyperplane in the sample space based on the training data set D, so as to maximize the <font color='red'>geometric margin</font>, and to assign instances to different In the class, the equation $w*x+b=0$ corresponding to the separation hyperplane, which is determined by the normal vector w and the intercept b.\n",
    "   \n",
    "<center>$w=(w1,w2,...)$  is the normal vector, which determines the direction of the hyperplane <br />\n",
    "$b$  is the displacement amount, which determines the distance between the hyperplane and the origin <br /></center>\n",
    "The second goal is to find the corresponding decision function, and to divide the unknown data through the hyperplane:<br />\n",
    "$$f(x)=sign(wx+b)$$\n",
    "According to the theoretical proof, the hyperplane that maximizes the geometric margin is unique, and the intuitive explanation for maximizing the margin is: finding the hyperplane with the largest geometric margin for the training data set means classifying the training data with a sufficiently large degree of certainty. The geometric margin of the plane about each sample point should be greater than $\\gamma$. The constrained optimization problem of the maximal margin separating hyperplane is given below:<br />\n",
    "<center>$\\underset {w,b}{max}$&nbsp;&nbsp;$\\gamma$<br /><br />\n",
    "$s.t. \\frac{y_i~(wx_i+b)}{||w||}$ $\\geq$ $\\widehat{\\gamma}_i$\n",
    "</center>\n",
    "According to the relationship between geometric margin and function margin, the above formula can be rewritten as:<br />\n",
    "<center>$\\underset {w,b}{max}$&nbsp;&nbsp;$\\frac{\\widehat{\\gamma}}{||w||}$<br /><br />\n",
    "$s.t. y_i$($w$$\\cdot$$x_i$>$+b$) $\\geq$ $\\widehat{\\gamma}$\n",
    "</center>\n",
    "$\\widehat{\\gamma}$ is the function margin, which is affected by $w$ and $b$, that is, how many times it expands at the same time, how many times the function margin will expand, but divide it by a $||w||$ After that, the multiple of the result is reduced, so no matter how the function margin changes, the geometric margin is not affected, then we can also set the function margin to a fixed value $1$, which can greatly simplify our models and calculations. Then our goal is to maximize $\\frac{1}{||w||}$, considering the problem of later calculation, convert it into a more general form, which is equivalent to minimizing $\\frac{ 1}{2}$$||w||^2$, so we get the final optimization constraint problem:\n",
    "<center>$\\underset {w,b}{min}$&nbsp;&nbsp;$\\frac{1}{2}$$||w||^2$<br /><br />\n",
    "$s.t. y_i$($w$$\\cdot$$x_i$$+b$)-1 $\\geq$ 0 \n",
    "</center>\n",
    "The maximum margin method is to use the above to solve the constrained optimization problem.\n",
    "\n",
    "\n",
    " \n",
    "### 2.Linear Support Vector Machines and Soft Margin Max\n",
    "In the above, we mainly discussed the linearly separable support vector machines, but in reality, the data is often linearly inseparable, that is, there are noise points in the data, so that it is not completely linearly separable, or approximately linearly separable. In this case, This situation is called soft margin maximization, that is, the linear support vector machine, and the linearly separable support vector machine can be regarded as a special case of the linear support vector machines. <br /><br />\n",
    "Assuming a training dataset $T=${$(x_1,y_1),(x_2,y_2),...,(x_N,y_N)$} on a feature space, where $x_i \\in X = R ^n $, $y_i = {+1,-1}, i=1,2,...,N$, $x_i$ is the $i$ feature vector, $y_i$ is the class label of $x_i$ . Also assume that the training dataset is not linearly separable. Usually, there are some outliers in the training data. After removing these outliers, the set of most of the remaining sample points is linearly separable.<br /><br /> \n",
    "Introduce a slack variable $\\xi_i \\geq 0$ to each sample point, so that the function plus the slack variable is greater than or equal to $1$, and the constraint becomes<br />\n",
    "<center>$y_i$($w$$\\cdot$$x_i$$+b$) $\\geq$ $1 - \\xi_i$</center><br />\n",
    "The objective function becomes:<br />\n",
    "<center>$\\frac{1}{2}$$||w||^2$ + $C\\sum_{i=1}^N \\xi_i$</center><br />\n",
    "Among them, $C$ is the penalty parameter, $C>0$, which is generally determined by the practical application problem. When $C$ increases, the penalty for misclassification is greater, and when $C$ decreases, the penalty for misclassification decreases. And how to solve the linear inseparable support vector machines? The learning dual algorithm is used here, that is, the Lagrangian duality algorithm is used to solve it.<br /><br /> \n",
    "Construct the Lagrangian function:<br />\n",
    "<center>\n",
    "$L(\\omega,b,\\xi,\\alpha,\\mu)$ = $\\frac{1}{2}$$||w||^2$ + $C\\sum_{i=1}^N \\xi_i$ + $\\sum_{i=1}^N \\alpha_i(1-\\xi_i-y_i(\\omega x_i+b))$ - $\\sum_{i=1}^N \\mu_i\\xi_i$\n",
    "</center><br />\n",
    "Where $\\alpha_i \\geq 0$,$\\mu_i \\geq 0$. The original problem is the problem of finding the minimum and maximum ($\\underset{\\alpha}{max}$ $\\underset {\\omega,b,\\xi}{min}$\n",
    "$L(\\omega,b,\\xi,\\alpha,\\mu)$), Then its dual problem is to find the maximum and minimum problems, that is:<br />\n",
    "<center>\n",
    "$\\underset {\\alpha}{min}$&nbsp;$\\frac{1}{2}$$\\sum_{i=1}^N$$\\sum_{j=1}^N$$\\alpha_i \\alpha_j y_i y_j$($x_i$$\\cdot$$x_j$) $-$ $\\sum_{i=1}^N \\alpha_i$ <br>\n",
    "$s.t. \\sum_{i=1}^N \\alpha_i y_i$ $=$ 0, <br>\n",
    "$0 \\leq \\alpha_i \\leq C , i = 1,2,...,N$ <br>\n",
    "</center>\n",
    "The optimal solution can be obtained $\\alpha^*=(\\alpha_1^*,\\alpha_2^*,...,\\alpha_N^*)^T$, and then bring $\\alpha^*$ into the expression to get Solution for $w$<br />\n",
    "<center> $w^* = \\sum_{i=1}^N \\alpha_i^* y_i x_i$ </center><br />\n",
    "Select a component of $\\alpha^*$ $\\alpha_j^*$ to satisfy the condition $0<\\alpha_j^*<C$, calculate<br />\n",
    "<center> $b^* = y_i - \\sum_{i=1}^N y_i \\alpha_i^* (x_i \\cdot y_i)$ </center><br />\n",
    "Finally, the separating hyperplane can be obtained:<br />\n",
    "<center> $w^* \\cdot x + b^* = 0 $ </center><br />\n",
    "The classification decision function is:<br />\n",
    "<center> $f(x) = sign (w^* \\cdot x + b^* ) $ </center><br />\n",
    "\n",
    "\n",
    "### 3.Nonlinear Support Vector Machines\n",
    "For samples that are linearly inseparable in a limited dimensional vector space, we can map them to a higher-dimensional vector space, and then learn the support vector machine through the maximum interval. This is the nonlinear support vector machines, simple The understanding is:<br> \n",
    "<center><b>Nonlinear SVM = Kernel Skill + Linear SVM</b>.</center>\n",
    "The key of nonlinear kernel function is to map the linearly inseparable sample data in the input space to the linearly separable feature space. The quality of the feature space directly affects the effect of SVM, so the choice of kernel function is an extremely important issue. Here are some commonly used kernel functions:\n",
    "    \n",
    "#### 1. Linear kernel function<br />\n",
    "<center>$k(x_i , x_j) = x_i^T x_j$ </center><br />\n",
    "There is no need to specify parameters when used, it directly calculates the inner product of the two input vectors. For the samples transformed by the linear kernel function, the feature space and the input space coincide, which means that the samples are not mapped to a higher dimensional space.\n",
    "\n",
    "#### 2. Polynomial Kernel Function\n",
    "<center>$k(x_i , x_j) =(\\gamma x_i^T x_j + r)d，\\gamma > 0，d \\geq 1$</center><br />\n",
    "It is necessary to specify three parameters $\\gamma, r and d$, which is an unstable kernel, which is suitable for the case where the data is normalized and standardized.<br />\n",
    "\n",
    "#### 3. RBF kernel function\n",
    "<center>$k(x_i , x_j) =exp(-\\gamma||x_i-x_j||^2)，\\gamma > 0$</center><br />\n",
    "RBF kernel, also known as Gaussian kernel, is a family of kernel functions. It maps samples in the input space to a higher-dimensional space in a non-linear fashion, so it can handle the non-linear relationship between class labels and sample attributes. It has a parameter $\\gamma$ This parameter is very critical! If the setting is too large, the entire RBF kernel will degenerate like a linear kernel, and the ability to non-linearly project to higher dimensions will be weakened; if the setting is too small, the influence of noise in the sample will be too large, thus interfering with the final training model. Effect. <br />   \n",
    "\n",
    "#### 4. Sigmoid kernel function\n",
    "<center>$k(x_i , x_j) =tanh(\\gamma x_i^T x_j + r)$</center><br />\n",
    "Sigmoid kernel function is derived from neural network and is widely used in machine learning and deep learning. It has two parameters $\\gamma and r$, and the parameter settings should be appropriate. <br />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Part 2 Code Example\n",
    "> Note: See the file SVM.ipynb for the complete code\n",
    "### 1.Hard-Margin Support Vector Machines\n",
    "#### a) Import the library and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2226d6e7-e1c6-4fe9-9c70-cee50bb6b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl  # Drawing function\n",
    "from sklearn import svm  #Import the svm module in the sklearn library\n",
    "\n",
    "# Here 40 points are randomly generated as a linearly separable training dataset\n",
    "X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\n",
    "# Samples of classes 0 and 1 20 points per class, Y is a column vector of 40 rows and 1 column\n",
    "Y = [0] * 20 + [1] * 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d96b90-2e3a-4a1c-99fc-9c642279c37b",
   "metadata": {},
   "source": [
    "#### b) Create a Support Vector Machines Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674291c6-e94f-4cde-8312-1403a7e2bd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build svm model\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf2170d-4785-4d81-a9a7-079aa2b5aea9",
   "metadata": {},
   "source": [
    "#### c) Define the decision boundary function\n",
    "Drawing decision boundary function according to support vector machine SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d12675b-4af5-4dce-b09c-ddfc85b2b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc_decision_boundary(clf,xmin,xmax):\n",
    "    w = clf.coef_[0] # model weights\n",
    "    b = clf.intercept_[0] # model offset\n",
    "    \n",
    "    # The decision boundary function (division hyperplane equation) satisfies: w0x0 + w1x1 + b = 0, convert it into a point-slope equation\n",
    "    # => ：x1 = -(w0/w1)x0 - (b/w1)\n",
    "    xx = np.linspace(xmin,xmax) # xmin starting point, xmax is the end point\n",
    "    yy = -(w[0] / w[1]) * xx - b / w[1] \n",
    "    \n",
    "    margin = 1 / w[1]  #The maximum spacing of support vectors is twice that\n",
    "    # Draw and divide two lines parallel to the hyperplane and passing through the support vector (same slope, different intercept)\n",
    "    yy_down = yy - margin\n",
    "    yy_up = yy + margin\n",
    "    \n",
    "    # get support vectors and print\n",
    "    svs = clf.support_vectors_\n",
    "    print(\"support_vectors_ : \",svs)\n",
    "    \n",
    "    # Draw decision boundaries, marginal planes and sample points\n",
    "    pl.plot(xx, yy, 'k-') \n",
    "    pl.plot(xx, yy_down, 'k--')\n",
    "    pl.plot(xx, yy_up, 'k--')\n",
    "    \n",
    "    # Circle the support vectors\n",
    "    pl.scatter(svs[:, 0], svs[:, 1],s=80, facecolors='#FFAAAA')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cfeb6d-f1de-452e-bcd0-58d51f2123ae",
   "metadata": {},
   "source": [
    "#### d) Draw decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32322af9-f968-4191-b3be-271cda208c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_decision_boundary(clf,-5,5)  # call function\n",
    "pl.scatter(X[:, 0], X[:, 1], c=Y, cmap=pl.cm.Paired) # plot all points\n",
    "pl.axis([-5, 5, -5, 5])  # coordinate limits\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54903d33-ee20-48da-9319-1137a886abfa",
   "metadata": {},
   "source": [
    "#### f) Experimental summary\n",
    "The above experiment achieved the classification of the largest interval by building a self-built <font color=\"red\">linearly separable</font> sample set. From the image, it can be clearly seen that the samples are well separated, and the two dotted lines is the maximum interval, focusing on marking points as support vectors. Let's predict a new point (2, 4) and see its class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118d7a2-ee5a-432b-8ec7-23f3f9332033",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.predict([[2,4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd48b5d-7c80-414d-867f-0359976b5a15",
   "metadata": {},
   "source": [
    "### 2.Soft margin support vector machines\n",
    "According to the above principle, if there are some outliers in the linearly separable data, the trained model cannot achieve a better effect. In this case, the function of soft interval is reflected, that is, the parameters in the SVM model Influence on training results, such as parameter penalty parameter C .\n",
    "#### a) Import the library and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb99296a-5a6e-49f5-9db0-a82c6eacd454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl  \n",
    "import matplotlib.pyplot as pl\n",
    "from sklearn import svm  \n",
    "from sklearn import datasets # Datasets that come with sklearn\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:,(2,3)]   # Take petal length and petal width as features\n",
    "Y = (iris[\"target\"] == 2).astype(np.float64) # virginica as a label, turn it into a binary classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a740b7-594a-4133-b014-ea2f5106a16e",
   "metadata": {},
   "source": [
    "#### b) Create and train a support vector machines model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e9d27f-3dc6-4f91-b01e-45fecbb436e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear', C=1, random_state=50)\n",
    "clf.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707a7f4c-9148-4875-a10e-8aeb3e84d8a9",
   "metadata": {},
   "source": [
    "#### c) Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2fcafc-90ad-4977-afaa-c5c60a15507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict([[2,4],[3.5,5.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bf78b9-6a0c-46f4-a8e5-d0bc93482232",
   "metadata": {},
   "source": [
    "#### d) Create two models\n",
    "Two SVM models are generated, and different penalty factors $C$ are selected in the creation stage to compare the impact of the penalty factors on the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cfec8c-beb0-4dda-87de-670e96f30a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = svm.SVC(kernel='linear', C=1, random_state=50)\n",
    "clf2 = svm.SVC(kernel='linear', C=100, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cba0bd9-089b-40d7-8ba4-2f8869e7b161",
   "metadata": {},
   "source": [
    "#### e) Train two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7516865-33d7-4dd7-8a23-f33b984d6dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2873b0d-3fe4-4963-80a8-840d76ee4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ca8d26-a7b7-4730-817f-0943e522714a",
   "metadata": {},
   "source": [
    "#### f) Define the decision boundary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2367f0f6-d97b-45a9-8b9f-5a3d88db6231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc_decision_boundary(clf,xmin,xmax):\n",
    "    w = clf.coef_[0] \n",
    "    b = clf.intercept_[0] \n",
    "    \n",
    "    xx = np.linspace(xmin,xmax) \n",
    "    yy = -(w[0] / w[1]) * xx - b / w[1] \n",
    "    \n",
    "    margin = 1 / w[1] \n",
    "\n",
    "    yy_down = yy - margin\n",
    "    yy_up = yy + margin\n",
    "    \n",
    "    svs = clf.support_vectors_\n",
    "    \n",
    "    pl.plot(xx, yy, 'k-') \n",
    "    pl.plot(xx, yy_down, 'k--')\n",
    "    pl.plot(xx, yy_up, 'k--')\n",
    "    \n",
    "    pl.scatter(svs[:, 0], svs[:, 1],s=80, facecolors='#FFAAAA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9630db-fdb8-48df-90c7-74cbb083ee16",
   "metadata": {},
   "source": [
    "#### g) Draw decision boundaries\n",
    "Draw two decision boundaries and compare how the penalty factor affects the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6df753-5c0a-4570-8684-9acaa01c5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rc(\"axes\",labelsize=14)\n",
    "fig,axes = pl.subplots(ncols=2,figsize=(15,4.1),sharey=True)\n",
    "pl.style.use(\"ggplot\")\n",
    "\n",
    "pl.sca(axes[0])\n",
    "# Plot points with class 1\n",
    "pl.plot(X[:,0][Y==1],X[:,1][Y==1],\"om\")\n",
    "# Plot points with class 0\n",
    "pl.plot(X[:,0][Y==0],X[:,1][Y==0],\"ob\")\n",
    "# Draw the first SVM decision boundary\n",
    "svc_decision_boundary(clf1,4,6)\n",
    "pl.title(\"$C = {}$\".format(clf1.C),fontsize=16)\n",
    "pl.axis([4,6,1,3])\n",
    "\n",
    "pl.sca(axes[1])\n",
    "# Plot points with class 1\n",
    "pl.plot(X[:,0][Y==1],X[:,1][Y==1],\"om\")\n",
    "# Plot points with class 0\n",
    "pl.plot(X[:,0][Y==0],X[:,1][Y==0],\"ob\")\n",
    "# Draw the first SVM decision boundary\n",
    "svc_decision_boundary(clf2,4,6)\n",
    "pl.title(\"$C = {}$\".format(clf2.C),fontsize=16)\n",
    "pl.axis([4,6,1,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbe502c-1db1-4e0d-9f32-82abc5291746",
   "metadata": {},
   "source": [
    "#### h) Experimental summary\n",
    "Compare the effect of different $C$ values:<br />\n",
    "* When the left $C$ takes a small value, that is, the training is not so strict, so the interval is much larger, and at the same time, many instances will appear in the interval (to prevent overfitting).<br />\n",
    "* When the right $C$ gets a larger value, the classification of the classifier is relatively more accurate, but it also gets a smaller interval.<br />\n",
    "* Of course, the value of $C$ is a parameter in the classification problem, which can be selected in other ways, and the specific problem can be assigned a specific value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ce0ad-fea0-4402-9d67-a42800e6b570",
   "metadata": {},
   "source": [
    "### 3.Nonlinear Support Vector Machines\n",
    "#### a) Import the library and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19831861-5ed1-4fa6-bf68-f93e3e147538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl \n",
    "import matplotlib.pyplot as pl\n",
    "from sklearn import svm  \n",
    "from sklearn.datasets import make_moons\n",
    "X,y = make_moons(n_samples=100,noise=0.15,random_state=42) # Self-built crescent data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d248fa18-0ef9-4c11-80d6-387da0d712ba",
   "metadata": {},
   "source": [
    "#### b) Data plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55548295-8619-410b-a691-aee4c8037178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot():\n",
    "    pl.scatter(X[y==0,0],X[y==0,1],c='r',marker='*',label='label0')\n",
    "    pl.scatter(X[y==1,0],X[y==1,1],c='b',marker='^',label='label1')\n",
    "    pl.legend()\n",
    "plot()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca774f3-5f18-4023-972c-978582262dc5",
   "metadata": {},
   "source": [
    "#### c) Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59de9c27-6134-4fa4-b264-56c27373bab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='rbf',C=10)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c1738-14bd-4749-85be-85c34558fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the range where the data is located\n",
    "x_min,x_max = X[:,0].min()-1,X[:,0].max()+1\n",
    "y_min,y_max = X[:,1].min()-1,X[:,1].max()+1\n",
    "\n",
    "# Use meshgrid to get grid matrix\n",
    "#arange(start, end, step), similar to range(), but also without the end value. but returns an array object\n",
    "xx,yy = np.meshgrid(np.arange(x_min,x_max,0.02),np.arange(y_min,y_max,0.02))\n",
    "x_new = np.c_[xx.ravel(),yy.ravel()] # reval flattens the data and converts multi-dimensional data into 1-dimensional\n",
    "print(type(x_new))\n",
    "z = clf.predict(x_new)\n",
    "z = z.reshape(xx.shape)# transform array\n",
    "# draw a contour map\n",
    "cs = pl.contourf(xx,yy,z)# z stands for height, and the displayed color is different for different heights\n",
    "plot()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5687ff-d3c8-4a80-b396-5080eb35d21e",
   "metadata": {},
   "source": [
    "#### d) Self-built test set to verify its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5522a4a-0071-4ead-81d5-2489a527de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_moons(n_samples=10000,noise=0.15,random_state=42)\n",
    "clf.predict(X)\n",
    "print(\"Accuracy：\",clf.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51df0cf-b51b-4159-a5db-856f34360a73",
   "metadata": {},
   "source": [
    "The correct rate of the training set has reached more than 0.99, which has reached a high level.\n",
    "## Part 3 Task\n",
    "1. Briefly talk about the main idea of SVM?\n",
    "2. Use the make_circles() method in Sklearn to generate circular-like training datasets, draw data and decision boundary graphics, and build a test set to verify the accuracy. Hint:<br>\n",
    "\n",
    "+ function : \n",
    ">X_circle,Y_circle=make_circles(n_samples=400,noise=0.1,factor=0.1)<br>\n",
    "+ parameter : \n",
    "> n_samples：Set the sample size; <br>\n",
    "noise: Set the noise, if it is small, it will be more concentrated; <br>\n",
    "factor：0 < double < 1 defaults 0.8, scale factor between inner and outer circles; <br>\n",
    "random_state: Set random parameters (section).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
